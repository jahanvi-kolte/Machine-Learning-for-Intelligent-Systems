{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Project 3: Na&iuml;ve Bayes and the Perceptron</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"nb.png\" width=\"200px\" />\n",
    "    </center>\n",
    "      <p><cite><center>\"All models are wrong, but some are useful.\"<br>\n",
    "       -- George E.P. Box\n",
    "      </center></cite></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "<!--AÃ°albrandr-->\n",
    "\n",
    "<p>A&eth;albrandr is visiting America from Norway and has been having the hardest time distinguishing boys and girls because of the weird American names like Jack and Jane.  This has been causing lots of problems for A&eth;albrandr when he goes on dates. When he heard that Cornell has a Machine Learning class, he asked that we help him identify the gender of a person based on their name to the best of our ability.  In this project, you will implement Na&iuml;ve Bayes to predict if a name is male or female. </p>\n",
    "\n",
    "<strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.\n",
    "\n",
    "Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.\n",
    "\n",
    "<p><strong>Evaluation:</strong> Your code will be autograded for technical\n",
    "correctness and--on some assignments--speed. Please <em>do not</em> change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. Furthermore, <em>any code not surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags will not be run by the autograder</em>. However, the correctness of your implementation -- not the autograder's output -- will be the final judge of your score.  If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.\n",
    "\n",
    "<p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
    "\n",
    "<p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href=\"https://piazza.com/class/iyag4nk2rsxsv\">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Of boys and girls </h3>\n",
    "\n",
    "<p> Take a look at the files <code>girls.train</code> and <code>boys.train</code>. For example with the unix command <pre>cat girls.train</pre> \n",
    "<pre>\n",
    "...\n",
    "Addisyn\n",
    "Danika\n",
    "Emilee\n",
    "Aurora\n",
    "Julianna\n",
    "Sophia\n",
    "Kaylyn\n",
    "Litzy\n",
    "Hadassah\n",
    "</pre>\n",
    "Believe it or not, these are all more or less common girl names. The problem with the current file is that the names are in plain text, which makes it hard for a machine learning algorithm to do anything useful with them. We therefore need to transform them into some vector format, where each name becomes a vector that represents a point in some high dimensional input space. </p>\n",
    "\n",
    "<p>That is exactly what the following Python function <code>name2features</code> does: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "#</GRADED>\n",
    "import sys\n",
    "\n",
    "# add p03 folder\n",
    "sys.path.insert(0, './p03/')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def name2features(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lau\n",
      "ura\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#workarea\n",
    "bname='Laura'\n",
    "hashfeatures(bname,128,3)\n",
    "print(bname[:3])\n",
    "print(bname[-1*3:])\n",
    "fs = \"prefix\"+bname[:0]\n",
    "hash(fs )% 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "It reads every name in the given file and converts it into a 128-dimensional feature vector. </p> \n",
    "\n",
    "<p>Can you figure out what the features are? (Understanding how these features are constructed will help you later on in the competition.)<br></p>\n",
    "\n",
    "<p>We have provided you with a python function <code>genTrainFeatures</code>, which calls this script, transforms the names into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def genTrainFeatures(dimension=128, fix=3):\n",
    "    \"\"\"\n",
    "    function [x,y]=genTrainFeatures\n",
    "    \n",
    "    This function calls the python script \"name2features.py\" \n",
    "    to convert names into feature vectors and loads in the training data. \n",
    "    \n",
    "    \n",
    "    Output: \n",
    "    x: n feature vectors of dimensionality d [d,n]\n",
    "    y: n labels (-1 = girl, +1 = boy)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    Xgirls = name2features(\"girls.train\", B=dimension, FIX=fix)\n",
    "    Xboys = name2features(\"boys.train\", B=dimension, FIX=fix)\n",
    "    X = np.concatenate([Xgirls, Xboys])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(Xgirls)), np.ones(len(Xboys))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can call the following command to load in the features and the labels of all boys and girls names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X,Y = genTrainFeatures(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[[ 1  2  1 -1]\n",
      " [ 2 -1  4  2]\n",
      " [ 1  1  3  1]\n",
      " [-1 -1  4  2]]\n",
      "20\n",
      "20\n",
      "[1.09861229 0.69314718 1.60943791 0.69314718]\n",
      "[1.94591015 2.07944154 1.09861229 0.        ]\n",
      "[-0.84729786 -1.38629436  0.51082562  0.69314718]\n"
     ]
    }
   ],
   "source": [
    "#workarea\n",
    "print(X[1,:])\n",
    "y=[1,1,-1,-1,2]\n",
    "[label for label in y if label==1]\n",
    "np.ones((2,12))\n",
    "indices=[i for i, inp_label in enumerate(y) if inp_label == -1]\n",
    "\n",
    "a=np.matrix([[1,2,1,-1],[2,-1,4,2],[1,1,3,1],[-1,-1,4,2]])\n",
    "y1=[-1,1,1,-1]\n",
    "ni=[i for i, inp_label in enumerate(y1) if inp_label == -1]\n",
    "a[ni,:]\n",
    "print(a)\n",
    "print(a.sum())\n",
    "print(np.einsum('ij->',a))\n",
    "#print(np.log(a))\n",
    "p=[3,2,5,2]\n",
    "q=[7,8,3,1]\n",
    "print(np.log(p))\n",
    "print(np.log(q))\n",
    "print(np.log(p) - np.log(q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> The Na&iuml;ve Bayes Classifier </h3>\n",
    "\n",
    "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following questions will ask you to finish these functions in a pre-defined order. <br>\n",
    "<strong>As a general rule, you should avoid tight loops at all cost.</strong></p>\n",
    "<p>(a) Estimate the class probability P(Y) in \n",
    "<b><code>naivebayesPY</code></b>\n",
    ". This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPY(x,y):\n",
    "    \"\"\"\n",
    "    function [pos,neg] = naivebayesPY(x,y);\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (nxd)\n",
    "        y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "    pos: probability p(y=1)\n",
    "    neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    n = len(y)\n",
    "    ## fill in code here\n",
    "    pos=len([label for label in y if label==1])/n\n",
    "    neg=len([label for label in y if label==-1])/n\n",
    "    \n",
    "    return pos,neg\n",
    "#</GRADED>\n",
    "\n",
    "pos,neg = naivebayesPY(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(b) Estimate the conditional probabilities P(X|Y) in \n",
    "<b><code>naivebayesPXY</code></b>\n",
    ".  Use a <b>multinomial</b> distribution as model. This will return the probability vectors  for all features given a class label.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPXY(x,y):\n",
    "    \"\"\"\n",
    "    function [posprob,negprob] = naivebayesPXY(x,y);\n",
    "    \n",
    "    Computation of P(X|Y)\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (nxd)\n",
    "        y : n labels (-1 or +1) (nx1)\n",
    "    \n",
    "    Output:\n",
    "    posprob: probability vector of p(x|y=1) (1xd)\n",
    "    negprob: probability vector of p(x|y=-1) (1xd)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    n, d = x.shape\n",
    "    x = np.concatenate([x, np.ones((2,d))])\n",
    "    y = np.concatenate([y, [-1,1]])\n",
    "    n, d = x.shape\n",
    "    \n",
    "    ## fill in code here\n",
    "    \n",
    "    #Get indexes where y=1\n",
    "    pos_indices = [i for i, inp_label in enumerate(y) if inp_label == 1]\n",
    "    #Get indexes where y=-1\n",
    "    neg_indices = [i for i, inp_label in enumerate(y) if inp_label == -1]\n",
    "    \n",
    "    #input vectors labelled positive\n",
    "    inp_pos=x[pos_indices,:]\n",
    "    \n",
    "    #input vectore labelled negative\n",
    "    inp_neg=x[neg_indices,:]\n",
    "    \n",
    "    #Initialize output \n",
    "    posprob=np.zeros((1,d))\n",
    "    negprob=np.zeros((1,d))\n",
    "    \n",
    "    #Sum over axis=0/individual features\n",
    "    posprob=(np.einsum('ij->j',inp_pos))/(np.einsum('ij->',inp_pos))\n",
    "    negprob=(np.einsum('ij->j',inp_neg))/(np.einsum('ij->',inp_neg))\n",
    "    \n",
    "    return posprob,negprob\n",
    "    \n",
    "#</GRADED>\n",
    "\n",
    "posprob,negprob = naivebayesPXY(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(c) Solve for the log ratio, $\\log\\left(\\frac{P(Y=1 | X = xtest)}{P(Y=-1|X= xtest)}\\right)$, using Bayes Rule.\n",
    " Implement this in \n",
    "<b><code>naivebayes</code></b>.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayes(x,y,xtest):\n",
    "    \"\"\"\n",
    "    function logratio = naivebayes(x,y);\n",
    "    \n",
    "    Computation of log P(Y|X=x1) using Bayes Rule\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1)\n",
    "    xtest: input vector of d dimensions (1xd)\n",
    "    \n",
    "    Output:\n",
    "    logratio: log (P(Y = 1|X=xtest)/P(Y=-1|X=xtest))\n",
    "    \"\"\"\n",
    "    \n",
    "    ## fill in code here\n",
    "    pos,neg = naivebayesPY(x,y)\n",
    "    posprob,negprob = naivebayesPXY(x,y)\n",
    "    \n",
    "    #Get features of test where value=1\n",
    "    test_feature_indices = [i for i, test_feature in enumerate(xtest) if test_feature >= 1]\n",
    "    \n",
    "    \n",
    "    poslog=np.einsum('i->',np.log(posprob[test_feature_indices]))+ np.log(pos)\n",
    "    neglog=np.einsum('i->',np.log(negprob[test_feature_indices]))+ np.log(neg)\n",
    "    \n",
    "    return poslog-neglog\n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "p = naivebayes(X,Y,X[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(d) NaÃ¯ve Bayes can also be written as a linear classifier. Implement this in \n",
    "<b><code>naivebayesCL</code></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesCL(x,y):\n",
    "    \"\"\"\n",
    "    function [w,b]=naivebayesCL(x,y);\n",
    "    Implementation of a Naive Bayes classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    y : n labels (-1 or +1)\n",
    "\n",
    "    Output:\n",
    "    w : weight vector of d dimensions\n",
    "    b : bias (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = x.shape\n",
    "    ## fill in code here\n",
    "    pos,neg = naivebayesPY(x,y)\n",
    "    posprob,negprob = naivebayesPXY(x,y)\n",
    "    \n",
    "    #Initialize output\n",
    "    w=np.zeros((1,d))\n",
    "    w=np.log(posprob)-np.log(negprob)\n",
    "    b=np.log(pos)-np.log(neg)\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "w,b = naivebayesCL(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(e) Implement \n",
    "<b><code>classifyLinear</code></b>\n",
    " that applies a linear weight vector and bias to a set of input vectors and outputs their predictions.  (You can use your answer from the previous project.)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def innerproduct(X,Z=None):\n",
    "    # function innerproduct(X,Z)\n",
    "    #\n",
    "    # Computes the inner-product matrix.\n",
    "    # Syntax:\n",
    "    # D=innerproduct(X,Z)\n",
    "    # Input:\n",
    "    # X: nxd data matrix with n vectors (rows) of dimensionality d\n",
    "    # Z: mxd data matrix with m vectors (rows) of dimensionality d\n",
    "    #\n",
    "    # Output:\n",
    "    # Matrix G of size nxm\n",
    "    # G[i,j] is the inner-product between vectors X[i,:] and Z[j,:]\n",
    "    #\n",
    "    # call with only one input:\n",
    "    # innerproduct(X)=innerproduct(X,X)\n",
    "    #\n",
    "    if Z is None: # case when there is only one input (X)\n",
    "        G=np.matmul(X,X.transpose())\n",
    "    else:  # case when there are two inputs (X,Z)\n",
    "        G=np.matmul(X,Z.transpose())\n",
    "    \n",
    "    return G\n",
    "#</GRADED>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 22.92%\n"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "def classifyLinear(x,w,b=0):\n",
    "    \"\"\"\n",
    "    function preds=classifyLinear(x,w,b);\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (nxd)\n",
    "    w : weight vector of d dimensions\n",
    "    b : bias (optional)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    ## fill in code here\n",
    "\n",
    "\n",
    "    w = w.reshape(1,-1)\n",
    "    \n",
    "    d=w.shape[1]\n",
    "    x=x.reshape(-1,d)\n",
    "    n=x.shape[0]\n",
    "    preds=np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if innerproduct(w,x[i,:])+b>=0:\n",
    "            preds[i]=1\n",
    "        else:\n",
    "            preds[i]=-1\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "print('Training error: %.2f%%' % (100 *(classifyLinear(X, w, b) != Y).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can now test your code with the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier ...\n",
      "Training error: 22.92%\n",
      "Please enter your name>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIMS = 128\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeatures(DIMS)\n",
    "print('Training classifier ...')\n",
    "w,b=naivebayesCL(X,Y)\n",
    "error = np.mean(classifyLinear(X,w,b) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))\n",
    "\n",
    "while True:\n",
    "    print('Please enter your name>')\n",
    "    yourname = input()\n",
    "    if len(yourname) < 1:\n",
    "        break\n",
    "    xtest = name2features(yourname,B=DIMS,LoadFile=False)\n",
    "    pred = classifyLinear(xtest,w,b)[0]\n",
    "    if pred > 0:\n",
    "        print(\"%s, I am sure you are a nice boy.\\n\" % yourname)\n",
    "    else:\n",
    "        print(\"%s, I am sure you are a nice girl.\\n\" % yourname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17,\n",
       " 19,\n",
       " 22,\n",
       " 23,\n",
       " 32,\n",
       " 33,\n",
       " 39,\n",
       " 40,\n",
       " 44,\n",
       " 46,\n",
       " 47,\n",
       " 52,\n",
       " 55,\n",
       " 65,\n",
       " 66,\n",
       " 72,\n",
       " 76,\n",
       " 79,\n",
       " 82,\n",
       " 83,\n",
       " 87,\n",
       " 93,\n",
       " 103,\n",
       " 124,\n",
       " 125,\n",
       " 132,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 147,\n",
       " 150,\n",
       " 152,\n",
       " 155,\n",
       " 170,\n",
       " 181,\n",
       " 190,\n",
       " 194,\n",
       " 200,\n",
       " 203,\n",
       " 205,\n",
       " 209,\n",
       " 210,\n",
       " 212,\n",
       " 218,\n",
       " 221,\n",
       " 224,\n",
       " 225,\n",
       " 230,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 237,\n",
       " 242,\n",
       " 246,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 254,\n",
       " 259,\n",
       " 267,\n",
       " 273,\n",
       " 278,\n",
       " 287,\n",
       " 296,\n",
       " 301,\n",
       " 304,\n",
       " 305,\n",
       " 320,\n",
       " 329,\n",
       " 330,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 346,\n",
       " 349,\n",
       " 351,\n",
       " 360,\n",
       " 362,\n",
       " 364,\n",
       " 368,\n",
       " 370,\n",
       " 379,\n",
       " 381,\n",
       " 385,\n",
       " 394,\n",
       " 398,\n",
       " 399,\n",
       " 408,\n",
       " 413,\n",
       " 419,\n",
       " 424,\n",
       " 426,\n",
       " 429,\n",
       " 432,\n",
       " 434,\n",
       " 441,\n",
       " 443,\n",
       " 444,\n",
       " 451,\n",
       " 457,\n",
       " 464,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 476,\n",
       " 478,\n",
       " 489,\n",
       " 493,\n",
       " 496,\n",
       " 497,\n",
       " 503,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 510,\n",
       " 515,\n",
       " 527,\n",
       " 529,\n",
       " 536,\n",
       " 539,\n",
       " 543,\n",
       " 544,\n",
       " 567,\n",
       " 570,\n",
       " 572,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 588,\n",
       " 589,\n",
       " 591,\n",
       " 592,\n",
       " 599,\n",
       " 603,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 618,\n",
       " 621,\n",
       " 624,\n",
       " 626,\n",
       " 632,\n",
       " 646,\n",
       " 647,\n",
       " 657,\n",
       " 662,\n",
       " 663,\n",
       " 668,\n",
       " 674,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 687,\n",
       " 695,\n",
       " 702,\n",
       " 711,\n",
       " 717,\n",
       " 721,\n",
       " 731,\n",
       " 734,\n",
       " 736,\n",
       " 739,\n",
       " 743,\n",
       " 747,\n",
       " 749,\n",
       " 754,\n",
       " 757,\n",
       " 761,\n",
       " 765,\n",
       " 769,\n",
       " 772,\n",
       " 785,\n",
       " 787,\n",
       " 791,\n",
       " 802,\n",
       " 804,\n",
       " 812,\n",
       " 815,\n",
       " 820,\n",
       " 822,\n",
       " 828,\n",
       " 829,\n",
       " 832,\n",
       " 834,\n",
       " 837,\n",
       " 840,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 852,\n",
       " 866,\n",
       " 872,\n",
       " 886,\n",
       " 889,\n",
       " 891,\n",
       " 892,\n",
       " 899,\n",
       " 902,\n",
       " 903,\n",
       " 912,\n",
       " 914,\n",
       " 917,\n",
       " 929,\n",
       " 937,\n",
       " 938,\n",
       " 946,\n",
       " 949,\n",
       " 952,\n",
       " 953,\n",
       " 968,\n",
       " 969,\n",
       " 973,\n",
       " 977,\n",
       " 983,\n",
       " 985,\n",
       " 1004,\n",
       " 1007,\n",
       " 1013,\n",
       " 1018,\n",
       " 1026,\n",
       " 1031,\n",
       " 1033,\n",
       " 1034,\n",
       " 1037,\n",
       " 1038,\n",
       " 1042,\n",
       " 1048,\n",
       " 1049,\n",
       " 1051,\n",
       " 1054,\n",
       " 1058,\n",
       " 1066,\n",
       " 1068,\n",
       " 1071,\n",
       " 1075,\n",
       " 1090,\n",
       " 1095,\n",
       " 1096,\n",
       " 1101,\n",
       " 1104,\n",
       " 1106,\n",
       " 1113,\n",
       " 1118,\n",
       " 1122,\n",
       " 1125,\n",
       " 1127,\n",
       " 1132,\n",
       " 1139,\n",
       " 1147,\n",
       " 1157,\n",
       " 1159,\n",
       " 1163,\n",
       " 1166,\n",
       " 1169,\n",
       " 1180,\n",
       " 1181,\n",
       " 1186,\n",
       " 1196,\n",
       " 1199]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List few misclassified samples\n",
    "preds=classifyLinear(X,w,b)\n",
    "indices=[i for i,x in enumerate(classifyLinear(X,w,b) != Y)  if x]\n",
    "indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Feature Extraction (Competition)</h3>\n",
    "\n",
    "<p>(e) (<b>optional</b>) As always, this programming project also includes a competition.  We will rank all submissions by how well your Na&iuml;ve Bayes classifier performs on a secret test set. If you want to improve your classifier modify <code>name2features2</code> below.   The automatic reader will use your Python script to extract features and train your classifier on the same names training set by calling the function with only one argument--the name of a file containing a list of names.  The given implementation is the same as the given <code>name2features</code> above.\n",
    "</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def hashfeatures(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    #nef - number of extra features\n",
    "    nef=0\n",
    "    baby=baby.lower()\n",
    "    \n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B-nef] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B-nef] = 1\n",
    "        \n",
    "    for m in range(1,len(baby)):\n",
    "        if baby[m-1] == baby[m]:\n",
    "            v[hash(\"repetition\"+ baby[m]+baby[m]) % B-nef] = 1\n",
    " \n",
    "    featurestring = \"vowel\" + (\"true\" if baby[-1] in \"aeiouy\"  else \"false\")\n",
    "    v[hash(featurestring) % B-nef] = 1\n",
    "    featurestring = \"vowelnumber\" + (\"true\" if np.sum(list(map(baby.count, \"aeiou\")))>=3 else \"false\")\n",
    "    v[hash(featurestring) % B-nef] = 1\n",
    "#     featurestring = \"twovowels\" + (\"true\" if baby[-1] in \"aeiou\" and baby[-2] in \"aeiou\"  else \"false\")\n",
    "#     v[hash(featurestring) % B-nef] = 1 \n",
    "#     middle = int(len(baby)/2)\n",
    "#     featurestring = \"middle\" + baby[middle]\n",
    "#     v[hash(featurestring) % B-nef] = 1\n",
    "#     featurestring = \"middle\" + baby[middle-1] + baby[middle]\n",
    "#     v[hash(featurestring) % B-nef] = 1\n",
    "    \n",
    "        #How to avoid collision? Avoiding collision increses accuracy tried(100000) 0.799 acc\n",
    "        \n",
    "        #improve hash function\n",
    "        #feature extraction\n",
    "        #hyperparam tuning\n",
    "        #dimension redn\n",
    "        \n",
    "        #sparse dataset is always a problem\n",
    "        #most significant letters\n",
    "        \n",
    "        #fill up the matrix more\n",
    "    \n",
    "    #Taking first two and second two\n",
    "    #featurestring = \"prefix\" + baby[:2]\n",
    "    #v[hash(featurestring) % B-nef] = 2\n",
    "    #featurestring = \"prefix\" + baby[1:3]\n",
    "    #v[hash(featurestring) % B-nef] = 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Check the last char - vowel or y\n",
    "    #v[B-nef]= 2 if baby[-1] in \"aeiouy\"  else 0\n",
    "        \n",
    "    #Check if last two letters are vowels\n",
    "    #v[B-nef+1]= 2 if baby[-1] in \"aeiou\" and baby[-2] in \"aeiou\" else 0\n",
    "\n",
    "    #Check if last two letters are consonants\n",
    "        \n",
    "    #Number of vowels >=3 usually for girls, assign more weight\n",
    "    #v[B-nef+2] = 2 if np.sum(list(map(baby.lower().count, \"aeiou\")))>=3 else 0\n",
    "        \n",
    "    #ends with vowel\n",
    "    #v[B-nef+1]= 2 if baby[-1] in \"aeiouy\" else 0\n",
    "        \n",
    "    #does not end with vowel\n",
    "    #v[B-nef+2]= 2 if baby[-1] not in \"aeiou\" else 0\n",
    "        \n",
    "    #begins with consonant and ends with vowel\n",
    "    #v[B-nef+3]= 1 if baby.isalpha() and baby[0] not in \"aeiou\" and baby[-1] in \"aeiou\" else 0\n",
    "        \n",
    "    #begins with vowel and ends with consonant\n",
    "    #v[B-nef+4]= 2 if baby.isalpha() and baby[0] in \"aeiou\" and baby[-1] not in \"aeiou\" else 0\n",
    "    \n",
    "    \n",
    "        \n",
    "    return v\n",
    "\n",
    "def hashfeatures2(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    #extra features location\n",
    "    #ef_loc=FIX*26\n",
    "    ef_loc=2*FIX*26\n",
    "    baby=baby.lower()\n",
    "    \n",
    "    for m in range(FIX):\n",
    "        #Start hashing characters from right\n",
    "        v[m*26 + (ord(baby[-(m+1)]) - ord('a'))]=2\n",
    "    \n",
    "    for m in range(FIX):\n",
    "        #Start hashing characters from left\n",
    "        v[(m+FIX)*26 + (ord(baby[m]) - ord('a'))]=1\n",
    "    \n",
    "    #Check the last char - vowel or y\n",
    "    v[ef_loc]= 1 if baby[-1] in \"aeiouy\"  else 0\n",
    "    \n",
    "    #Check if last two letters are vowels\n",
    "    v[ef_loc+1]= 1 if baby[-2] in \"aeiou\" else 0\n",
    "    \n",
    "    #Number of vowels >=3 usually for girls, assign more weight\n",
    "    #v[ef_loc+2] = 1 if np.sum(list(map(baby.lower().count, \"aeiou\")))>=2 else 0\n",
    "    \n",
    "    #begins with consonant and ends with vowel\n",
    "    #v[ef_loc+2]= 1 if baby.isalpha() and baby[0] not in \"aeiou\" and baby[-1] in \"aeiou\" else 0\n",
    "    \n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "def hashfeatures3(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    #number of extra features\n",
    "    \n",
    "    \n",
    "    \n",
    "    baby=baby.lower()\n",
    "    \n",
    "    v[ ( ord(baby[-1]) - ord('a') ) % 26] = 1\n",
    "    v[ ( ord(baby[-2]) - ord('a') ) % 26] = v[ ( ord(baby[-2]) - ord('a') ) % 26] + 1\n",
    "    \n",
    "    #last char vowel\n",
    "    #Check the last char - vowel or y\n",
    "    v[27]= 1 if baby[-1] in \"aeiouy\"  else 0\n",
    "    v[27]= v[27] + (1 if baby[-2] in \"aeiou\"  else 0)\n",
    "    \n",
    "    return v\n",
    "\n",
    "def name2features2(filename, B=20000, FIX=5, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    #Increasing training data set\n",
    "    #babynames=babynames\n",
    "    \n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "        #X[i,:] = hashfeatures3(babynames[i], B, FIX)\n",
    "        \n",
    "        \n",
    "    return X\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genTrainFeaturesComp(dimension=128, fix=3):\n",
    "    \"\"\"\n",
    "    function [x,y]=genTrainFeatures\n",
    "    \n",
    "    This function calls the python script \"name2features.py\" \n",
    "    to convert names into feature vectors and loads in the training data. \n",
    "    \n",
    "    \n",
    "    Output: \n",
    "    x: n feature vectors of dimensionality d [d,n]\n",
    "    y: n labels (-1 = girl, +1 = boy)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    Xgirls = name2features2(\"girls.train\", B=dimension, FIX=fix)\n",
    "    Xboys = name2features2(\"boys.train\", B=dimension, FIX=fix)\n",
    "    X = np.concatenate([Xgirls, Xboys])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(Xgirls)), np.ones(len(Xboys))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Credits</h4>\n",
    "  Parts of this webpage were copied from or heavily inspired by John DeNero's and Dan Klein's (awesome) <a href=\"http://ai.berkeley.edu/project_overview.html\">Pacman class</a>. The name classification idea originates from <a href=\"http://nickm.com\">Nick Montfort</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 1 0 0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#changed B\n",
    "baby=\"Jahanvi\"\n",
    "a=list(map(baby.lower().count, \"aeiou\"))\n",
    "print(*a)\n",
    "np.sum(a)\n",
    "print(1 if baby[-1].lower() not in \"aeiou\" else 0)\n",
    "print(1 if baby.isalpha() and baby[0] not in \"aeiou\" and baby[-1] in \"aeiou\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier ...\n",
      "Training error: 18.75%\n"
     ]
    }
   ],
   "source": [
    "DIMS = 150\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeaturesComp(DIMS)\n",
    "print('Training classifier ...')\n",
    "w,b=naivebayesCL(X,Y)\n",
    "error = np.mean(classifyLinear(X,w,b) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JK', 'SN', 'AK', 'JK', 'SN', 'AK']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babynames =['JK','SN','AK']\n",
    "np.repeat(babynames,2)\n",
    "babynames*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"jahanvi\"\n",
    "ord('v') - ord('i')\n",
    "len(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
